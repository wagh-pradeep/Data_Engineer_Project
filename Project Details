----------------------------- Project 1 : G** Rapid POV ---------------------------------------------------

-- Rapid POV Detailed Project Flow :
1. Data is coming from Kafka and mongoDB based on unique bpc number.
2. using Dataproc, GCS bucket, Bigquery, Composer, Airflow transfer environment and data from Azure Databricks to GCP.

- How we solved:

1. Create container and mount point:
- Set env / widgets for bpc (bpc is unique customer code), bucket name, env etc.
	bpc = 20093
- Importing necesseary libraries:
	from google.coud import storage
- Create function using env to create folder in bucket based on bpc number.
- Created Same function for files 
- Import Secret manager client library
	from google.cloud import secretmanager

2. Create Master Tables:
- Set env
- create spark session using delta table properties.
- 



----------------------------- Project 2 : Tr** Rapid POV ---------------------------------------------------

-- Rapid POV Detailed Project Flow :
1. client gaves us delta tables from Azure enviromnment to GCP bucket.
2. we created temp view on that delta file and created delta table on top of the view.
3. we have created databases, tables etc.
4. we faced many challenges like NVL replaced with coalesce, analyze not working in V2 table
5. for optimizaion we used:
		1. optimize write
		2. optimize compact
6. build pipelines in composer > airflow for that we write DAGs (Dags means .py file where we contains upstream and downstream notebooks to execution)

-- Table names:
	1. factsales
	2. factOpenOrders
	3. VendorProductRanked
	4. DimCoupon
	5. DimProduct
	
-- Challenges

-- Services Used:
	1. databricks
	2. Dataproc
	3. Composer
	4. GCP Storage
	5. Airflow
	6. 

-- Working areas:
	1. Created TDD Documentation for client
	2. Developing Setup notebooks in Sandbox Folder which helps to build Infrastructure.

-- Table Size & No of Records:
    1. 6TB+
    2. 60 Billion+

-- No of Tables:
    1. 50+

------------------------------- CICD Rapid POV ---------------------------------------------------

We are creating scala jar using sbt and java jar using sbt/mvn to run on dataproc batches.
we are also creating python wheel packages to run on dataproc and databricks.

 